{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, pdb, pickle, argparse, shutil, yaml, torch, math, time, pdb, datetime, pickle, sys\n",
    "import utils #file\n",
    "from solver_encoder import Solver \n",
    "from data_loader import pathSpecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "import torch.nn.functional as F\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(adam_init=0.0001, autovc_ckpt='/homes/bdoc3/my_data/autovc_data/vte-autovc/model_saves/TrueVtesNoCd16Freq16Neck/ckpts/ckpt_310000.pth.tar', batch_size=1, chunk_num=6, chunk_seconds=0.5, ckpt_freq=10000, ckpt_model='', data_dir='/homes/bdoc3/my_data/autovc_data/vte-autovc/model_saves', device=device(type='cuda', index=0), dim_emb=256, dim_neck=16, dim_pre=512, emb_ckpt='/homes/bdoc3/phonDet/results/newStandardAutovcSpmelParamsUnnormLatent64Out256/best_epoch_checkpoint.pth.tar', exclude_list=['m1_', 'm4_', 'm5_', 'm6_', 'm7_', 'm8_', 'm9_', 'm11_', 'f1_', 'f3_', 'f5_', 'f6_', 'f7_', 'f8_', 'f9_'], file_name='TrueVtesNoCd16Freq16Neck', freq=16, lambda_cd=1, len_crop=192, log_step=10, num_iters=500000, one_hot=False, prnt_loss_weight=1.0, psnt_loss_weight=1.0, spec_freq=10000, spmel_dir='/homes/bdoc3/my_data/phonDet/spmel_autovc_params_unnormalized', train_size=21, use_ckpt_config=False, vte_ckpt='/homes/bdoc3/phonDet/results/newStandardAutovcSpmelParamsUnnormLatent64Out256/best_epoch_checkpoint.pth.tar', which_cuda=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tailor config, define other\n",
    "save_as = 'egs_metadata.pkl'\n",
    "model_name = 'TrueVtesNoCd16Freq16Neck' #just for using models config file and vte_model\n",
    "cudnn.benchmark = True\n",
    "use_avg_vte = False\n",
    "convert_style = True #if you plan to just resynthesize the examples or actually convert the styles\n",
    "use_meta_list = True\n",
    "ckpt_iters = 310000\n",
    "autovc_model_saves_dir = '/homes/bdoc3/my_data/autovc_data/vte-autovc/model_saves/'\n",
    "autovc_model_dir = autovc_model_saves_dir + model_name\n",
    "config = pickle.load(open(autovc_model_dir +'/config.pkl','rb'))\n",
    "config.batch_size = 1\n",
    "config.autovc_ckpt = autovc_model_dir +'/ckpts/ckpt_' +str(ckpt_iters) +'.pth.tar'\n",
    "avg_embs = np.load(os.path.dirname(config.emb_ckpt) +'/averaged_embs.npy')\n",
    "config.vte_ckpt = '/homes/bdoc3/phonDet/results/newStandardAutovcSpmelParamsUnnormLatent64Out256/best_epoch_checkpoint.pth.tar'\n",
    "# additional config attrs\n",
    "style_names = ['belt','lip_trill','straight','vocal_fry','vibrato','breathy']\n",
    "singer_names = ['m1_','m2_','m3_','m4_','m5_','m6_','m7_','m8_','m9_','m10_','m11_','f1_','f2_','f3_','f4_','f5_','f6_','f7_','f8_','f9_']\n",
    "test_names = pickle.load(open(os.path.dirname(config.emb_ckpt) +'/config_params.pkl', 'rb')).test_list.split(' ')\n",
    "train_names = [item for item in singer_names if item not in test_names]\n",
    "\n",
    "config.exclude_list = train_names\n",
    "\n",
    "male_idx = range(0,11)\n",
    "female_idx = range(11,20)\n",
    "config.device = torch.device(f'cuda:{config.which_cuda}' if torch.cuda.is_available() else 'cpu')\n",
    "with open(config.spmel_dir +'/spmel_params.yaml') as File:\n",
    "    spmel_params = yaml.load(File, Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2_arpeggios_belt_a.npy\n",
      "f2_arpeggios_belt_e.npy\n",
      "f2_arpeggios_belt_i.npy\n",
      "f2_arpeggios_belt_o.npy\n",
      "f2_arpeggios_belt_u.npy\n",
      "f2_arpeggios_breathy_a.npy\n",
      "f2_arpeggios_breathy_e.npy\n",
      "f2_arpeggios_breathy_i.npy\n",
      "f2_arpeggios_breathy_o.npy\n",
      "f2_arpeggios_breathy_u.npy\n",
      "f2_arpeggios_lip_trill_a.npy\n",
      "f2_arpeggios_lip_trill_e.npy\n",
      "f2_arpeggios_lip_trill_i.npy\n",
      "f2_arpeggios_lip_trill_o.npy\n",
      "f2_arpeggios_lip_trill_u.npy\n",
      "f2_arpeggios_straight_a.npy\n",
      "f2_arpeggios_straight_e.npy\n",
      "f2_arpeggios_straight_i.npy\n",
      "f2_arpeggios_straight_o.npy\n",
      "f2_arpeggios_straight_u.npy\n",
      "f2_arpeggios_vibrato_a.npy\n",
      "f2_arpeggios_vibrato_e.npy\n",
      "f2_arpeggios_vibrato_i.npy\n",
      "f2_arpeggios_vibrato_o.npy\n",
      "f2_arpeggios_vibrato_u.npy\n",
      "f2_arpeggios_vocal_fry_a.npy\n",
      "f2_arpeggios_vocal_fry_e.npy\n",
      "f2_arpeggios_vocal_fry_i.npy\n",
      "f2_arpeggios_vocal_fry_o.npy\n",
      "f2_arpeggios_vocal_fry_u.npy\n",
      "f2_scales_belt_a.npy\n",
      "f2_scales_belt_e.npy\n",
      "f2_scales_belt_i.npy\n",
      "f2_scales_belt_o.npy\n",
      "f2_scales_belt_u.npy\n",
      "f2_scales_breathy_a.npy\n",
      "f2_scales_breathy_e.npy\n",
      "f2_scales_breathy_i.npy\n",
      "f2_scales_breathy_o.npy\n",
      "f2_scales_breathy_u.npy\n",
      "f2_scales_lip_trill_a.npy\n",
      "f2_scales_lip_trill_e.npy\n",
      "f2_scales_lip_trill_i.npy\n",
      "f2_scales_lip_trill_o.npy\n",
      "f2_scales_lip_trill_u.npy\n",
      "f2_scales_straight_a.npy\n",
      "f2_scales_straight_e.npy\n",
      "f2_scales_straight_i.npy\n",
      "f2_scales_straight_o.npy\n",
      "f2_scales_straight_u.npy\n",
      "f2_scales_vibrato_a.npy\n",
      "f2_scales_vibrato_e.npy\n",
      "f2_scales_vibrato_i.npy\n",
      "f2_scales_vibrato_o.npy\n",
      "f2_scales_vibrato_u.npy\n",
      "f2_scales_vocal_fry_a.npy\n",
      "f2_scales_vocal_fry_e.npy\n",
      "f2_scales_vocal_fry_i.npy\n",
      "f2_scales_vocal_fry_o.npy\n",
      "f2_scales_vocal_fry_u.npy\n",
      "f4_arpeggios_belt_a.npy\n",
      "f4_arpeggios_belt_e.npy\n",
      "f4_arpeggios_belt_i.npy\n",
      "f4_arpeggios_belt_o.npy\n",
      "f4_arpeggios_belt_u.npy\n",
      "f4_arpeggios_breathy_a.npy\n",
      "f4_arpeggios_breathy_e.npy\n",
      "f4_arpeggios_breathy_i.npy\n",
      "f4_arpeggios_breathy_o.npy\n",
      "f4_arpeggios_breathy_u.npy\n",
      "f4_arpeggios_lip_trill_a.npy\n",
      "f4_arpeggios_lip_trill_e.npy\n",
      "f4_arpeggios_lip_trill_i.npy\n",
      "f4_arpeggios_lip_trill_o.npy\n",
      "f4_arpeggios_lip_trill_u.npy\n",
      "f4_arpeggios_straight_a.npy\n",
      "f4_arpeggios_straight_e.npy\n",
      "f4_arpeggios_straight_i.npy\n",
      "f4_arpeggios_straight_o.npy\n",
      "f4_arpeggios_straight_u.npy\n",
      "f4_arpeggios_vibrato_a.npy\n",
      "f4_arpeggios_vibrato_e.npy\n",
      "f4_arpeggios_vibrato_i.npy\n",
      "f4_arpeggios_vibrato_o.npy\n",
      "f4_arpeggios_vibrato_u.npy\n",
      "f4_arpeggios_vocal_fry_a.npy\n",
      "f4_arpeggios_vocal_fry_e.npy\n",
      "f4_arpeggios_vocal_fry_i.npy\n",
      "f4_arpeggios_vocal_fry_o.npy\n",
      "f4_arpeggios_vocal_fry_u.npy\n",
      "f4_scales_belt_a.npy\n",
      "f4_scales_belt_e.npy\n",
      "f4_scales_belt_i.npy\n",
      "f4_scales_belt_o.npy\n",
      "f4_scales_belt_u.npy\n",
      "f4_scales_breathy_a.npy\n",
      "f4_scales_breathy_e.npy\n",
      "f4_scales_breathy_i.npy\n",
      "f4_scales_breathy_o.npy\n",
      "f4_scales_breathy_u.npy\n",
      "f4_scales_lip_trill_a.npy\n",
      "f4_scales_lip_trill_e.npy\n",
      "f4_scales_lip_trill_i.npy\n",
      "f4_scales_lip_trill_o.npy\n",
      "f4_scales_lip_trill_u.npy\n",
      "f4_scales_straight_a.npy\n",
      "f4_scales_straight_e.npy\n",
      "f4_scales_straight_i.npy\n",
      "f4_scales_straight_o.npy\n",
      "f4_scales_straight_u.npy\n",
      "f4_scales_vibrato_a.npy\n",
      "f4_scales_vibrato_e.npy\n",
      "f4_scales_vibrato_i.npy\n",
      "f4_scales_vibrato_o.npy\n",
      "f4_scales_vibrato_u.npy\n",
      "f4_scales_vocal_fry_a.npy\n",
      "f4_scales_vocal_fry_e.npy\n",
      "f4_scales_vocal_fry_i.npy\n",
      "f4_scales_vocal_fry_o.npy\n",
      "f4_scales_vocal_fry_u.npy\n",
      "m10_arpeggios_belt_a.npy\n",
      "m10_arpeggios_belt_e.npy\n",
      "m10_arpeggios_belt_i.npy\n",
      "m10_arpeggios_belt_o.npy\n",
      "m10_arpeggios_belt_u.npy\n",
      "m10_arpeggios_breathy_a.npy\n",
      "m10_arpeggios_breathy_e.npy\n",
      "m10_arpeggios_breathy_i.npy\n",
      "m10_arpeggios_breathy_o.npy\n",
      "m10_arpeggios_breathy_u.npy\n",
      "m10_arpeggios_lip_trill_a.npy\n",
      "m10_arpeggios_lip_trill_e.npy\n",
      "m10_arpeggios_lip_trill_i.npy\n",
      "m10_arpeggios_lip_trill_o.npy\n",
      "m10_arpeggios_lip_trill_u.npy\n",
      "m10_arpeggios_straight_a.npy\n",
      "m10_arpeggios_straight_e.npy\n",
      "m10_arpeggios_straight_i.npy\n",
      "m10_arpeggios_straight_o.npy\n",
      "m10_arpeggios_straight_u.npy\n",
      "m10_arpeggios_vibrato_a.npy\n",
      "m10_arpeggios_vibrato_e.npy\n",
      "m10_arpeggios_vibrato_i.npy\n",
      "m10_arpeggios_vibrato_o.npy\n",
      "m10_arpeggios_vibrato_u.npy\n",
      "m10_arpeggios_vocal_fry_a.npy\n",
      "m10_arpeggios_vocal_fry_e.npy\n",
      "m10_arpeggios_vocal_fry_i.npy\n",
      "m10_arpeggios_vocal_fry_o.npy\n",
      "m10_arpeggios_vocal_fry_u.npy\n",
      "m10_scales_belt_a.npy\n",
      "m10_scales_belt_e.npy\n",
      "m10_scales_belt_i.npy\n",
      "m10_scales_belt_o.npy\n",
      "m10_scales_belt_u.npy\n",
      "m10_scales_breathy_a.npy\n",
      "m10_scales_breathy_e.npy\n",
      "m10_scales_breathy_i.npy\n",
      "m10_scales_breathy_o.npy\n",
      "m10_scales_breathy_u.npy\n",
      "m10_scales_lip_trill_a.npy\n",
      "m10_scales_lip_trill_e.npy\n",
      "m10_scales_lip_trill_i.npy\n",
      "m10_scales_lip_trill_o.npy\n",
      "m10_scales_lip_trill_u.npy\n",
      "m10_scales_straight_a.npy\n",
      "m10_scales_straight_e.npy\n",
      "m10_scales_straight_i.npy\n",
      "m10_scales_straight_o.npy\n",
      "m10_scales_straight_u.npy\n",
      "m10_scales_vibrato_a.npy\n",
      "m10_scales_vibrato_e.npy\n",
      "m10_scales_vibrato_i.npy\n",
      "m10_scales_vibrato_o.npy\n",
      "m10_scales_vibrato_u.npy\n",
      "m10_scales_vocal_fry_a.npy\n",
      "m10_scales_vocal_fry_e.npy\n",
      "m10_scales_vocal_fry_i.npy\n",
      "m10_scales_vocal_fry_o.npy\n",
      "m10_scales_vocal_fry_u.npy\n",
      "m2_arpeggios_belt_a.npy\n",
      "m2_arpeggios_belt_e.npy\n",
      "m2_arpeggios_belt_i.npy\n",
      "m2_arpeggios_belt_o.npy\n",
      "m2_arpeggios_belt_u.npy\n",
      "m2_arpeggios_breathy_a.npy\n",
      "m2_arpeggios_breathy_e.npy\n",
      "m2_arpeggios_breathy_i.npy\n",
      "m2_arpeggios_breathy_o.npy\n",
      "m2_arpeggios_breathy_u.npy\n",
      "m2_arpeggios_lip_trill_a.npy\n",
      "m2_arpeggios_lip_trill_e.npy\n",
      "m2_arpeggios_lip_trill_i.npy\n",
      "m2_arpeggios_lip_trill_o.npy\n",
      "m2_arpeggios_lip_trill_u.npy\n",
      "m2_arpeggios_straight_a.npy\n",
      "m2_arpeggios_straight_e.npy\n",
      "m2_arpeggios_straight_i.npy\n",
      "m2_arpeggios_straight_o.npy\n",
      "m2_arpeggios_straight_u.npy\n",
      "m2_arpeggios_vibrato_a.npy\n",
      "m2_arpeggios_vibrato_e.npy\n",
      "m2_arpeggios_vibrato_i.npy\n",
      "m2_arpeggios_vibrato_o.npy\n",
      "m2_arpeggios_vibrato_u.npy\n",
      "m2_arpeggios_vocal_fry_a.npy\n",
      "m2_arpeggios_vocal_fry_e.npy\n",
      "m2_arpeggios_vocal_fry_i.npy\n",
      "m2_arpeggios_vocal_fry_o.npy\n",
      "m2_arpeggios_vocal_fry_u.npy\n",
      "m2_scales_belt_a.npy\n",
      "m2_scales_belt_e.npy\n",
      "m2_scales_belt_i.npy\n",
      "m2_scales_belt_o.npy\n",
      "m2_scales_belt_u.npy\n",
      "m2_scales_breathy_a.npy\n",
      "m2_scales_breathy_e.npy\n",
      "m2_scales_breathy_i.npy\n",
      "m2_scales_breathy_o.npy\n",
      "m2_scales_breathy_u.npy\n",
      "m2_scales_lip_trill_a.npy\n",
      "m2_scales_lip_trill_e.npy\n",
      "m2_scales_lip_trill_i.npy\n",
      "m2_scales_lip_trill_o.npy\n",
      "m2_scales_lip_trill_u.npy\n",
      "m2_scales_straight_a.npy\n",
      "m2_scales_straight_e.npy\n",
      "m2_scales_straight_i.npy\n",
      "m2_scales_straight_o.npy\n",
      "m2_scales_straight_u.npy\n",
      "m2_scales_vibrato_a.npy\n",
      "m2_scales_vibrato_e.npy\n",
      "m2_scales_vibrato_i.npy\n",
      "m2_scales_vibrato_o.npy\n",
      "m2_scales_vibrato_u.npy\n",
      "m2_scales_vocal_fry_a.npy\n",
      "m2_scales_vocal_fry_e.npy\n",
      "m2_scales_vocal_fry_i.npy\n",
      "m2_scales_vocal_fry_o.npy\n",
      "m2_scales_vocal_fry_u.npy\n",
      "m3_arpeggios_belt_a.npy\n",
      "m3_arpeggios_belt_e.npy\n",
      "m3_arpeggios_belt_i.npy\n",
      "m3_arpeggios_belt_o.npy\n",
      "m3_arpeggios_belt_u.npy\n",
      "m3_arpeggios_breathy_a.npy\n",
      "m3_arpeggios_breathy_e.npy\n",
      "m3_arpeggios_breathy_i.npy\n",
      "m3_arpeggios_breathy_o.npy\n",
      "m3_arpeggios_breathy_u.npy\n",
      "m3_arpeggios_lip_trill_a.npy\n",
      "m3_arpeggios_lip_trill_e.npy\n",
      "m3_arpeggios_lip_trill_i.npy\n",
      "m3_arpeggios_lip_trill_o.npy\n",
      "m3_arpeggios_lip_trill_u.npy\n",
      "m3_arpeggios_straight_a.npy\n",
      "m3_arpeggios_straight_e.npy\n",
      "m3_arpeggios_straight_i.npy\n",
      "m3_arpeggios_straight_o.npy\n",
      "m3_arpeggios_straight_u.npy\n",
      "m3_arpeggios_vibrato_a.npy\n",
      "m3_arpeggios_vibrato_e.npy\n",
      "m3_arpeggios_vibrato_i.npy\n",
      "m3_arpeggios_vibrato_o.npy\n",
      "m3_arpeggios_vibrato_u.npy\n",
      "m3_arpeggios_vocal_fry_a.npy\n",
      "m3_arpeggios_vocal_fry_e.npy\n",
      "m3_arpeggios_vocal_fry_i.npy\n",
      "m3_arpeggios_vocal_fry_o.npy\n",
      "m3_arpeggios_vocal_fry_u.npy\n",
      "m3_scales_belt_a.npy\n",
      "m3_scales_belt_e.npy\n",
      "m3_scales_belt_i.npy\n",
      "m3_scales_belt_o.npy\n",
      "m3_scales_belt_u.npy\n",
      "m3_scales_breathy_a.npy\n",
      "m3_scales_breathy_e.npy\n",
      "m3_scales_breathy_i.npy\n",
      "m3_scales_breathy_o.npy\n",
      "m3_scales_breathy_u.npy\n",
      "m3_scales_lip_trill_a.npy\n",
      "m3_scales_lip_trill_e.npy\n",
      "m3_scales_lip_trill_i.npy\n",
      "m3_scales_lip_trill_o.npy\n",
      "m3_scales_lip_trill_u.npy\n",
      "m3_scales_straight_a.npy\n",
      "m3_scales_straight_e.npy\n",
      "m3_scales_straight_i.npy\n",
      "m3_scales_straight_o.npy\n",
      "m3_scales_straight_u.npy\n",
      "m3_scales_vibrato_a.npy\n",
      "m3_scales_vibrato_e.npy\n",
      "m3_scales_vibrato_i.npy\n",
      "m3_scales_vibrato_o.npy\n",
      "m3_scales_vibrato_u.npy\n",
      "m3_scales_vocal_fry_a.npy\n",
      "m3_scales_vocal_fry_e.npy\n",
      "m3_scales_vocal_fry_i.npy\n",
      "m3_scales_vocal_fry_o.npy\n",
      "m3_scales_vocal_fry_u.npy\n"
     ]
    }
   ],
   "source": [
    "# setup dataloader, models\n",
    "vocalSet = pathSpecDataset(config, spmel_params)\n",
    "vocalSet_loader = DataLoader(vocalSet, batch_size=config.batch_size, shuffle=True, drop_last=False)\n",
    "vte = utils.setup_vte(config, spmel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/homes/bdoc3/vte-autovc', '/homes/bdoc3/my_data/autovc_data', '', '/import/linux/python/3.7.7/lib/python3.7/site-packages', '/import/linux/python/3.7.7/lib/python37.zip', '/import/linux/python/3.7.7/lib/python3.7', '/import/linux/python/3.7.7/lib/python3.7/lib-dynload', '/homes/bdoc3/.local/lib/python3.7/site-packages', '/homes/bdoc3/.local/lib/python3.7/site-packages/IPython/extensions', '/homes/bdoc3/.ipython']\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(1, '/homes/bdoc3/my_data/autovc_data') # usually the cwd is priority, so index 1 is good enough for our purposes here\n",
    "print(sys.path)\n",
    "from hparams import hparams\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_male = True\n",
    "num_examples = 8\n",
    "use_unseen = True\n",
    "counter = 0\n",
    "data_iter = iter(vocalSet_loader)\n",
    "if use_meta_list == True:\n",
    "    example_meta_list = pickle.load(open(save_as, 'rb'))\n",
    "else:\n",
    "    example_meta_list = []\n",
    "\n",
    "# start the spmel_emb generation cycle\n",
    "while counter < num_examples:\n",
    "    x_real, org_style_idx, singer_idx = next(data_iter)\n",
    "    x_real = x_real.to(config.device)\n",
    "    original_spmel = x_real[0].cpu().detach().numpy()\n",
    "    if find_male == True:\n",
    "        gender_idx = male_idx\n",
    "    else:\n",
    "        gender_idx = female_idx\n",
    "    # if this example is the gender we're looking for\n",
    "    if singer_idx in gender_idx:\n",
    "        find_male = not find_male\n",
    "        counter += 1\n",
    "        # get source embedding\n",
    "        if use_avg_vte == True:\n",
    "            emb_org = torch.tensor(avg_embs[org_style_idx]).to(config.device).unsqueeze(0)\n",
    "        else:\n",
    "            x_real_chunked = x_real.view(x_real.shape[0]*config.chunk_num, x_real.shape[1]//config.chunk_num, -1)\n",
    "            pred_style_idx, all_tensors = vte(x_real_chunked)\n",
    "            emb_org = all_tensors[-1]\n",
    "        example_meta_list.append((original_spmel, org_style_idx, singer_idx, emb_org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_meta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_as,'wb') as handle:\n",
    "    pickle.dump(example_meta_list,handle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvautovc",
   "language": "python",
   "name": "venvautovc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
