{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key variables\n",
    "from_num_iters=400000\n",
    "model_name = 'Default1Hot'\n",
    "one_hot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSION PERFORMS THE ACTUAL VOICE CONVERSION THAT HAPPENS AFTER A MODEL IS TRAINED,\n",
    "# SO WE'VE BEEN PROVIDED WITH A PRETRAINED AUTOVC MODEL TO DEMONSTRATE THIS\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from model_vc import Generator\n",
    "import random\n",
    "\n",
    "def pad_seq(x, base=32):\n",
    "    len_out = int(base * ceil(float(x.shape[0])/base))\n",
    "    len_pad = len_out - x.shape[0]\n",
    "    assert len_pad >= 0\n",
    "    return np.pad(x, ((0,len_pad),(0,0)), 'constant'), len_pad\n",
    "\n",
    "# open the data that was used to train the model - consists of id, embedding, uttrs file name\n",
    "all_training_data = pickle.load(open('./model_saves/' +model_name +'/training_meta_data.pkl', 'rb'))\n",
    "all_meta_data = pickle.load(open('all_meta_data.pkl', \"rb\"))\n",
    "one_hot_array = np.eye(len(all_training_data))[np.arange(len(all_training_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_spkr_list = []\n",
    "# metadata = pickle.load(open('metadata.pkl', \"rb\"))\n",
    "# for entry in metadata:\n",
    "#     meta_spkr_list.append(entry[0])\n",
    "# for entry in all_training_data:\n",
    "#     if entry[0] in meta_spkr_list:\n",
    "#         print(f'{entry[0]} from training is in metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MALES ['p226', 'p227', 'p232', 'p237', 'p241', 'p243', 'p245', 'p246', 'p247', 'p251', 'p252', 'p254', 'p255', 'p256', 'p258', 'p259', 'p260', 'p263', 'p270', 'p271', 'p272', 'p273', 'p274', 'p275', 'p278', 'p279', 'p281', 'p284', 'p285', 'p286', 'p287', 'p292', 'p298', 'p302', 'p304', 'p311', 'p315', 'p316', 'p326', 'p334', 'p345', 'p347', 'p360', 'p363', 'p364', 'p374', 'p376']\n",
      "FEMALES ['p225', 'p228', 'p229', 'p230', 'p231', 'p233', 'p234', 'p236', 'p238', 'p239', 'p240', 'p244', 'p248', 'p249', 'p250', 'p253', 'p257', 'p261', 'p262', 'p264', 'p265', 'p266', 'p267', 'p268', 'p269', 'p276', 'p277', 'p280', 'p282', 'p283', 'p288', 'p293', 'p294', 'p295', 'p297', 'p299', 'p300', 'p301', 'p303', 'p305', 'p306', 'p307', 'p308', 'p310', 'p312', 'p313', 'p314', 'p317', 'p318', 'p323', 'p329', 'p330', 'p333', 'p335', 'p336', 'p339', 'p340', 'p341', 'p343', 'p351', 'p361', 'p362', 's5  ']\n",
      "selected random_spkrs ['p364', 'p376', 'p293', 'p233']\n",
      "selected_speaker p364\n",
      "selected_speaker p376\n",
      "selected_speaker p293\n",
      "selected_speaker p233\n"
     ]
    }
   ],
   "source": [
    "# select random males and females from the training set\n",
    "selected_speaker_for_conversion = []\n",
    "num_spkrs_per_gender = 2\n",
    "spkrs_found = 0\n",
    "uttr_idx = '001'\n",
    "\n",
    "male_id_list = []\n",
    "female_id_list = []\n",
    "f = open(\"/import/c4dm-datasets/VCTK-Corpus-0.92/speaker-info.txt\", \"r\")\n",
    "# print(f.read())\n",
    "for i, x in enumerate(f):\n",
    "    if i==0:\n",
    "        continue\n",
    "    female_id_list.append(x[0:4]) if x[10] =='F' else male_id_list.append(x[0:4])\n",
    "print('MALES', male_id_list)\n",
    "print('FEMALES', female_id_list)\n",
    "gender_lists = [male_id_list, female_id_list]\n",
    "\n",
    "\n",
    "# collect list of training speaker IDs\n",
    "all_training_id = []\n",
    "for spkr_data in all_training_data:\n",
    "    all_training_id.append(spkr_data[0])\n",
    "all_training_id\n",
    "\n",
    "# randomly select males and females from dataset\n",
    "random_spkrs = []\n",
    "for gender_list in gender_lists:\n",
    "    while spkrs_found < num_spkrs_per_gender:\n",
    "        random_spkr = random.choice(gender_list)\n",
    "        melspec_path ='./spmel/' +random_spkr +'/' +random_spkr +'_' +uttr_idx +'_mic1.npy'\n",
    "        # check to see does path with random speaker and selected utterance exist\n",
    "        if os.path.exists(melspec_path):        \n",
    "            if one_hot==True:\n",
    "                # check that selected speaker is in training set\n",
    "                if random_spkr in all_training_id:\n",
    "                    random_spkrs.append(random_spkr)\n",
    "                    spkrs_found += 1\n",
    "            else:\n",
    "                random_spkrs.append(random_spkr)\n",
    "                spkrs_found += 1\n",
    "    spkrs_found = 0\n",
    "print('selected random_spkrs', random_spkrs)\n",
    "\n",
    "# save appropriate speaker metadata to a list\n",
    "\n",
    "for selected_speaker in random_spkrs:\n",
    "    print('selected_speaker', selected_speaker)\n",
    "    if one_hot==True:\n",
    "        for i, spkr_data in enumerate(all_training_data):\n",
    "            if selected_speaker == spkr_data[0]:\n",
    "#                 print(selected_speaker,'is in all_training_data')\n",
    "                spkr_id = spkr_data[0]\n",
    "                if one_hot==True:\n",
    "                    spkr_emb = one_hot_array[i]\n",
    "                else:\n",
    "                    spkr_emb = spkr_data[1]\n",
    "                melspec_path ='./spmel/' +spkr_id +'/' +spkr_id +'_' +uttr_idx +'_mic1.npy'\n",
    "                melspec = np.load(melspec_path)\n",
    "                selected_speaker_for_conversion.append((spkr_id, spkr_emb, melspec))    \n",
    "            else:\n",
    "                tester = 0\n",
    "#                 print(selected_speaker,'not in all_training_data')\n",
    "    else:\n",
    "        # need to get embeddings for\n",
    "        for spkr_data in all_meta_data:\n",
    "            if selected_speaker == spkr_data[0]:\n",
    "#                 print(selected_speaker,'is in all_meta_data')\n",
    "                spkr_id = spkr_data[0]\n",
    "                if one_hot==True:\n",
    "                    spkr_emb = one_hot_array[i]\n",
    "                else:\n",
    "                    spkr_emb = spkr_data[1]\n",
    "                melspec_path ='./spmel/' +spkr_id +'/' +spkr_id +'_' +uttr_idx +'_mic1.npy'\n",
    "                melspec = np.load(melspec_path)\n",
    "                selected_speaker_for_conversion.append((spkr_id, spkr_emb, melspec))\n",
    "            else:\n",
    "                tester = 0\n",
    "#                 print(selected_speaker,'not in all_metadata_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# if using one-hot, we can't use the metadata\n",
    "device = 'cuda:0'\n",
    "# G = Generator(32,256,512,32).eval().to(device)\n",
    "\n",
    "#NEW BIT\n",
    "if one_hot==False:\n",
    "    G = Generator(32,256,512,32).eval().to(device)\n",
    "else:\n",
    "    G = Generator(32,20,512,32).eval().to(device)\n",
    "\n",
    "g_checkpoint = torch.load('./model_saves/' +model_name +'/ckpts/ckpt_' +str(from_num_iters) +'.pth.tar')\n",
    "G.load_state_dict(g_checkpoint['model_state_dict'])\n",
    "\n",
    "################### Paper - Section 4.2. The Content Encoder #########################################################\n",
    "\n",
    "# spect_vc collects spectrogram information to be used later for spec2wav conversion via wavnet model\n",
    "spect_vc = []\n",
    "\n",
    "x_org_list = []\n",
    "\n",
    "# each sbmt_i has a speaker ID, a speaker embedding, and a spectrogram\n",
    "for sbmt_i in selected_speaker_for_conversion:\n",
    "    \n",
    "    # x origin - 80Mel spectrogram\n",
    "    x_org = sbmt_i[2]\n",
    "    x_org_list.append( ('{}'.format(sbmt_i[0]), x_org) )\n",
    "    x_org, len_pad = pad_seq(x_org)\n",
    "    # utterance origin is just padded spec in tensor form\n",
    "    uttr_org = torch.from_numpy(x_org[np.newaxis, :, :]).to(device)\n",
    "    # speaker embedding \n",
    "    if one_hot==True:\n",
    "        emb_org = torch.from_numpy(sbmt_i[1][np.newaxis, :]).to(device).float()\n",
    "    else:\n",
    "        emb_org = torch.from_numpy(sbmt_i[1][np.newaxis, :]).to(device)\n",
    "        \n",
    "    # for each entry in metadata, use it as the embedding target and use this in G\n",
    "    for sbmt_j in selected_speaker_for_conversion:\n",
    "        # embedding target represents S2 in the paper - the embedding provided by a pretrained Speaker Encoder\n",
    "        # I guess this converts each utterance from the data so that it matches the utterance of every speaker\n",
    "        # So if there were 4 utterances with different speakers, then this code will generate 4*4=16 conversions\n",
    "        if one_hot==True:\n",
    "            emb_trg = torch.from_numpy(sbmt_j[1][np.newaxis, :]).to(device).float()\n",
    "        else:\n",
    "            emb_trg = torch.from_numpy(sbmt_j[1][np.newaxis, :]).to(device)\n",
    "        # as this is conversion not training, no backprop/gradientCommunication needed here\n",
    "        with torch.no_grad():\n",
    "            # x_identic_psnt = target utterance, produced by the Generator\n",
    "            # Generator is fed an utterance and speaker \n",
    "            _, x_identic_psnt, _ = G(uttr_org, emb_org, emb_trg)\n",
    "            \n",
    "        if len_pad == 0:\n",
    "            # utterance target is the converted speech\n",
    "            uttr_trg = x_identic_psnt[0, 0, :, :].cpu().numpy()\n",
    "        else:\n",
    "            uttr_trg = x_identic_psnt[0, 0, :-len_pad, :].cpu().numpy()\n",
    "        # utterance is saved in a list, along with source and target ID\n",
    "        spect_vc.append( ('{}x{}'.format(sbmt_i[0], sbmt_j[0]), uttr_trg) )\n",
    "        \n",
    "        \n",
    "with open('results.pkl', 'wb') as handle:\n",
    "    pickle.dump(spect_vc, handle)\n",
    "\n",
    "with open('x_org.pkl', 'wb') as howdy:\n",
    "    pickle.dump(x_org_list, howdy)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/37888 [00:00<09:01, 69.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p364xp364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 6828/37888 [01:38<07:29, 69.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d6f1bb3c6243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#     librosa.output.write_wav(name+'.wav', waveform, sr=16000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_saves/'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/generated_wavs/'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_num_iters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'kIters.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_autovc/synthesis.py\u001b[0m in \u001b[0;36mwavegen\u001b[0;34m(model, c, tqdm)\u001b[0m\n\u001b[1;32m     67\u001b[0m         y_hat = model.incremental_forward(\n\u001b[1;32m     68\u001b[0m             \u001b[0minitial_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             log_scale_min=hparams.log_scale_min)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autovc/venvAutoVc/lib/python3.7/site-packages/wavenet_vocoder/wavenet.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, initial_input, c, g, T, test_inputs, tqdm, softmax, quantize, log_scale_min)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                     \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mskips\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autovc/venvAutoVc/lib/python3.7/site-packages/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, x, c, g)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autovc/venvAutoVc/lib/python3.7/site-packages/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x, c, g, is_incremental)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0msplitdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0msplitdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autovc/venvAutoVc/lib/python3.7/site-packages/wavenet_vocoder/conv.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autovc/venvAutoVc/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#takes approximately 1.5 hours\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pickle\n",
    "from synthesis import build_model\n",
    "from synthesis import wavegen\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# supposedly trained on the VCTK model\n",
    "device = torch.device(\"cuda\")\n",
    "model = build_model().to(device)\n",
    "checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "counter = 0\n",
    "spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
    "\n",
    "for spect in spect_vc:\n",
    "#     if counter == 16:\n",
    "#         break\n",
    "#     counter +=1\n",
    "#     if counter == 0:\n",
    "#         continue\n",
    "#     name = str(counter) + spect[0]\n",
    "    name = spect[0]\n",
    "    # this splits the numpy spectrograms in half to as to save processing time\n",
    "    c = spect[1]\n",
    "    print(name)\n",
    "    waveform = wavegen(model, c=c)\n",
    "#     librosa.output.write_wav(name+'.wav', waveform, sr=16000)\n",
    "    sf.write('./model_saves/' +model_name +'/generated_wavs/' +name +'_' +str(from_num_iters) +'kIters.wav', waveform, samplerate=16000)\n",
    "\n",
    "time_taken = time.time() - start_time\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     import os\n",
    "#     from IPython.display import Audio\n",
    "#     wavs=[]\n",
    "\n",
    "#     for file in os.listdir(\"./\"):\n",
    "#         if file.endswith(\".wav\"):\n",
    "#             wavs.append(file)\n",
    "\n",
    "#     for wav in wavs:\n",
    "#         print(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.xp225',\n",
       " array([[0.28224188, 0.20554663, 0.15181297, ..., 0.00463975, 0.00347378,\n",
       "         0.01703978],\n",
       "        [0.33337137, 0.23668179, 0.16679963, ..., 0.03454047, 0.02398722,\n",
       "         0.02410528],\n",
       "        [0.37502947, 0.2653952 , 0.19988462, ..., 0.05794184, 0.04958811,\n",
       "         0.04577218],\n",
       "        ...,\n",
       "        [0.6726218 , 0.7531079 , 0.77818435, ..., 0.3141644 , 0.30782175,\n",
       "         0.30555806],\n",
       "        [0.6982055 , 0.75408345, 0.7720312 , ..., 0.34722567, 0.34975088,\n",
       "         0.34997335],\n",
       "        [0.64091647, 0.6756678 , 0.7091046 , ..., 0.34009993, 0.35053512,\n",
       "         0.34477887]], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
    "spect_vc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_file = './0xp225.wav'\n",
    "\n",
    "Audio(data=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23040/23040 [04:59<00:00, 76.93it/s]\n",
      "  0%|          | 6/22784 [00:00<06:55, 54.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22784/22784 [04:55<00:00, 77.03it/s]\n",
      "  0%|          | 8/19200 [00:00<04:03, 78.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19200/19200 [04:09<00:00, 77.05it/s]\n",
      "  0%|          | 6/27904 [00:00<08:22, 55.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27904/27904 [06:03<00:00, 76.83it/s]\n"
     ]
    }
   ],
   "source": [
    "x_orgs = pickle.load(open('x_org.pkl', 'rb'))\n",
    "\n",
    "for x_org in x_orgs:\n",
    "    name = x_org[0]\n",
    "    c = x_org[1]\n",
    "    print(name)\n",
    "    waveform = wavegen(model, c=c)   \n",
    "#     librosa.output.write_wav(name+'.wav', waveform, sr=16000)\n",
    "    sf.write(name+'.wav', waveform, samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 91, 83, 82, 7, 63, 16, 11, 53, 97, 79, 95, 2, 72, 26, 18, 69, 31, 60, 50]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvautovc",
   "language": "python",
   "name": "venvautovc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
